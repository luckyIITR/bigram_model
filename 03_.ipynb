{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae57596-21ba-4627-8f3f-8face7394719",
   "metadata": {},
   "source": [
    "datasource : https://www.kaggle.com/datasets/ananysharma/indian-names-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f67efa2-36d9-42d8-ba65-e1bed0f5afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mahipal', 'nanhe', 'vashudev', 'nandni', 'divya']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a36ae8-7952-406b-9c3d-45078f4fd456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29823"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0f1ff9-e1c5-4d0f-9ae4-61ffbeaca649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping\n",
    "# Create sorted list of unique characters from all words\n",
    "unique_chars = sorted(set(''.join(words)))\n",
    "\n",
    "# Add the special character '.' at the beginning\n",
    "itos = ['.'] + unique_chars\n",
    "stoi = {ch: idx for idx, ch in enumerate(itos)}\n",
    "itos = {idx: ch for ch, idx in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830917b7-319c-4cc5-9eb3-4829e89dcd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d88108d-acbf-4ac9-9abf-4ac0a7741c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". m\n",
      "m a\n",
      "a h\n",
      "h i\n",
      "i p\n",
      "p a\n",
      "a l\n",
      "l .\n"
     ]
    }
   ],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62df5d58-90c0-40aa-9f21-10798a497a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 13,  1,  8,  9, 16,  1, 12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f2f1ba8-d165-4ff5-944c-80e49919bbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13,  1,  8,  9, 16,  1, 12,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7791c2f9-2d55-46b9-abe0-b0bfcb68f929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d08a9d-ca7d-414e-b324-3b6ab93f5f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84da2df5-ee92-4137-a8ff-b16acd67a45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1235074d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabb89c3-5b13-47ea-b97b-945022ccfb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3871],\n",
       "        [ 0.1512],\n",
       "        [ 0.7800],\n",
       "        [-0.6262],\n",
       "        [-1.0862],\n",
       "        [ 0.2015],\n",
       "        [ 0.7800],\n",
       "        [-1.0413]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight matrix, taking single neuron\n",
    "W = torch.randn((27, 1))\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44293034-d0d2-4f2d-95f0-3ae9ed539842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight matrix, taking single neuron\n",
    "W = torch.randn((27, 27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c587d36-892f-4555-8c14-45adaa156019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0245, 0.0233, 0.0126, 0.0156, 0.0400, 0.0174, 0.0193, 0.0178, 0.0403,\n",
       "         0.0606, 0.0432, 0.0036, 0.0153, 0.0207, 0.0493, 0.0314, 0.0145, 0.0037,\n",
       "         0.0546, 0.0074, 0.0555, 0.0167, 0.0709, 0.1825, 0.0200, 0.0940, 0.0454],\n",
       "        [0.0561, 0.0305, 0.0160, 0.0319, 0.0171, 0.0346, 0.0405, 0.0369, 0.0140,\n",
       "         0.0467, 0.0066, 0.0362, 0.2266, 0.0128, 0.0323, 0.0241, 0.0150, 0.0461,\n",
       "         0.0067, 0.0383, 0.1013, 0.0167, 0.0074, 0.0279, 0.0331, 0.0266, 0.0182],\n",
       "        [0.0217, 0.0319, 0.0078, 0.0722, 0.0086, 0.0040, 0.0169, 0.0159, 0.0336,\n",
       "         0.0357, 0.0164, 0.0696, 0.0248, 0.0209, 0.0586, 0.1428, 0.0172, 0.0122,\n",
       "         0.1093, 0.0382, 0.0322, 0.0121, 0.0886, 0.0502, 0.0290, 0.0202, 0.0093],\n",
       "        [0.0096, 0.0279, 0.0500, 0.0406, 0.0084, 0.0173, 0.0465, 0.0186, 0.0053,\n",
       "         0.0143, 0.3544, 0.0166, 0.0124, 0.0474, 0.0296, 0.0100, 0.0415, 0.0051,\n",
       "         0.0119, 0.0273, 0.0106, 0.0355, 0.0155, 0.0373, 0.0451, 0.0311, 0.0305],\n",
       "        [0.0449, 0.0737, 0.0258, 0.0266, 0.0079, 0.0134, 0.0257, 0.0617, 0.0452,\n",
       "         0.0830, 0.0597, 0.0152, 0.0378, 0.0186, 0.0862, 0.0021, 0.1263, 0.0045,\n",
       "         0.0100, 0.0247, 0.0156, 0.0325, 0.0223, 0.0170, 0.0240, 0.0838, 0.0118],\n",
       "        [0.0260, 0.0785, 0.0163, 0.0121, 0.0097, 0.2113, 0.0656, 0.0350, 0.0107,\n",
       "         0.0582, 0.0048, 0.0338, 0.0030, 0.0026, 0.0267, 0.0055, 0.0717, 0.1088,\n",
       "         0.0517, 0.0052, 0.0447, 0.0111, 0.0516, 0.0406, 0.0030, 0.0034, 0.0083],\n",
       "        [0.0217, 0.0319, 0.0078, 0.0722, 0.0086, 0.0040, 0.0169, 0.0159, 0.0336,\n",
       "         0.0357, 0.0164, 0.0696, 0.0248, 0.0209, 0.0586, 0.1428, 0.0172, 0.0122,\n",
       "         0.1093, 0.0382, 0.0322, 0.0121, 0.0886, 0.0502, 0.0290, 0.0202, 0.0093],\n",
       "        [0.0180, 0.0514, 0.0298, 0.0251, 0.0319, 0.0430, 0.0948, 0.0107, 0.0293,\n",
       "         0.0044, 0.0266, 0.0272, 0.0150, 0.0229, 0.0696, 0.0350, 0.0943, 0.0803,\n",
       "         0.0096, 0.0197, 0.0148, 0.0225, 0.0356, 0.0899, 0.0062, 0.0537, 0.0384]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() # equivalent N in part 1\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94260ec4-9c29-4b39-83b1-2bcffb1b99a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27]), tensor(1.0000))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].shape, probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d4bf5c-11da-42b1-aa09-a7743b0ce22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d400382d-9502-4a73-8870-6503c42f1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d593ee6e-7e9c-433a-8737-ef89987a97e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 27])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9df5de76-3028-4960-93bf-92d971a5c1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Bigram example 1: .m (indexes 0, 13)\n",
      "Input to the neural net: 0\n",
      "Output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "Label (actual next character): 13\n",
      "Probability assigned to correct character: 0.008208369836211205\n",
      "Log likelihood: -4.802600860595703\n",
      "Negative log likelihood: 4.802600860595703\n"
     ]
    }
   ],
   "source": [
    "# computing loss\n",
    "i = 0\n",
    "# Get input and label indices\n",
    "x = xs[i].item()\n",
    "y = ys[i].item()\n",
    "\n",
    "print('--------')\n",
    "print(f'Bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x}, {y})')\n",
    "print('Input to the neural net:', x)\n",
    "\n",
    "# Get predicted probabilities for the i-th input\n",
    "prob_dist = probs[i]\n",
    "print('Output probabilities from the neural net:', prob_dist)\n",
    "\n",
    "print('Label (actual next character):', y)\n",
    "\n",
    "# Probability assigned to the correct character\n",
    "p = prob_dist[y]\n",
    "print('Probability assigned to correct character:', p.item())\n",
    "\n",
    "# Compute log-likelihood and negative log-likelihood\n",
    "logp = torch.log(p)\n",
    "nll = -logp\n",
    "print('Log likelihood:', logp.item())\n",
    "print('Negative log likelihood:', nll.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f4ec24c-7b8c-489d-8100-f8c34f1748d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Bigram example 1: .m (indexes 0, 13)\n",
      "Input to the neural net: 0\n",
      "Output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "Label (actual next character): 13\n",
      "Probability assigned to correct character: 0.008208369836211205\n",
      "Log likelihood: -4.802600860595703\n",
      "Negative log likelihood: 4.802600860595703\n",
      "--------\n",
      "Bigram example 2: ma (indexes 13, 1)\n",
      "Input to the neural net: 13\n",
      "Output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "Label (actual next character): 1\n",
      "Probability assigned to correct character: 0.07367684692144394\n",
      "Log likelihood: -2.6080667972564697\n",
      "Negative log likelihood: 2.6080667972564697\n",
      "--------\n",
      "Bigram example 3: ah (indexes 1, 8)\n",
      "Input to the neural net: 1\n",
      "Output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "Label (actual next character): 8\n",
      "Probability assigned to correct character: 0.012505755759775639\n",
      "Log likelihood: -4.381566047668457\n",
      "Negative log likelihood: 4.381566047668457\n",
      "--------\n",
      "Bigram example 4: hi (indexes 8, 9)\n",
      "Input to the neural net: 8\n",
      "Output probabilities from the neural net: tensor([0.0494, 0.0201, 0.0839, 0.0340, 0.0921, 0.0082, 0.0105, 0.0295, 0.0119,\n",
      "        0.0166, 0.0070, 0.0816, 0.0044, 0.0151, 0.0085, 0.0211, 0.0185, 0.0781,\n",
      "        0.0125, 0.0467, 0.0437, 0.1675, 0.0167, 0.0246, 0.0407, 0.0521, 0.0049])\n",
      "Label (actual next character): 9\n",
      "Probability assigned to correct character: 0.016575153917074203\n",
      "Log likelihood: -4.099850654602051\n",
      "Negative log likelihood: 4.099850654602051\n",
      "--------\n",
      "Bigram example 5: ip (indexes 9, 16)\n",
      "Input to the neural net: 9\n",
      "Output probabilities from the neural net: tensor([0.0227, 0.1059, 0.0202, 0.0328, 0.0728, 0.0147, 0.0179, 0.0074, 0.0293,\n",
      "        0.0297, 0.0181, 0.0179, 0.0470, 0.0059, 0.0970, 0.0059, 0.0796, 0.0786,\n",
      "        0.0245, 0.1022, 0.0311, 0.0342, 0.0367, 0.0100, 0.0216, 0.0250, 0.0114])\n",
      "Label (actual next character): 16\n",
      "Probability assigned to correct character: 0.079643115401268\n",
      "Log likelihood: -2.5301997661590576\n",
      "Negative log likelihood: 2.5301997661590576\n",
      "--------\n",
      "Bigram example 6: pa (indexes 16, 1)\n",
      "Input to the neural net: 16\n",
      "Output probabilities from the neural net: tensor([0.0276, 0.0674, 0.0403, 0.0881, 0.0507, 0.0199, 0.0398, 0.0796, 0.0373,\n",
      "        0.0232, 0.0437, 0.0262, 0.0295, 0.0335, 0.0031, 0.0501, 0.0436, 0.0051,\n",
      "        0.0154, 0.0195, 0.0230, 0.0163, 0.0142, 0.0136, 0.1342, 0.0400, 0.0148])\n",
      "Label (actual next character): 1\n",
      "Probability assigned to correct character: 0.06744828820228577\n",
      "Log likelihood: -2.6963939666748047\n",
      "Negative log likelihood: 2.6963939666748047\n",
      "--------\n",
      "Bigram example 7: al (indexes 1, 12)\n",
      "Input to the neural net: 1\n",
      "Output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "Label (actual next character): 12\n",
      "Probability assigned to correct character: 0.09880126267671585\n",
      "Log likelihood: -2.3146448135375977\n",
      "Negative log likelihood: 2.3146448135375977\n",
      "--------\n",
      "Bigram example 8: l. (indexes 12, 0)\n",
      "Input to the neural net: 12\n",
      "Output probabilities from the neural net: tensor([0.0107, 0.0173, 0.0445, 0.0068, 0.0278, 0.0232, 0.0120, 0.0329, 0.0381,\n",
      "        0.0135, 0.0051, 0.0085, 0.0365, 0.0234, 0.0411, 0.0290, 0.1845, 0.0757,\n",
      "        0.0110, 0.0143, 0.0261, 0.1264, 0.0338, 0.0737, 0.0293, 0.0383, 0.0167])\n",
      "Label (actual next character): 0\n",
      "Probability assigned to correct character: 0.010696436278522015\n",
      "Log likelihood: -4.537844657897949\n",
      "Negative log likelihood: 4.537844657897949\n",
      "=========\n",
      "Average negative log likelihood (i.e. loss) = 3.496396064758301\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first 8 bigrams and compute their individual negative log-likelihoods\n",
    "nlls = torch.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    # Get input and label indices\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "\n",
    "    print('--------')\n",
    "    print(f'Bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x}, {y})')\n",
    "    print('Input to the neural net:', x)\n",
    "\n",
    "    # Get predicted probabilities for the i-th input\n",
    "    prob_dist = probs[i]\n",
    "    print('Output probabilities from the neural net:', prob_dist)\n",
    "\n",
    "    print('Label (actual next character):', y)\n",
    "\n",
    "    # Probability assigned to the correct character\n",
    "    p = prob_dist[y]\n",
    "    print('Probability assigned to correct character:', p.item())\n",
    "\n",
    "    # Compute log-likelihood and negative log-likelihood\n",
    "    logp = torch.log(p)\n",
    "    nll = -logp\n",
    "    print('Log likelihood:', logp.item())\n",
    "    print('Negative log likelihood:', nll.item())\n",
    "\n",
    "    # Store the result\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('Average negative log likelihood (i.e. loss) =', nlls.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d361787-e690-4019-b627-c4e9d8a83c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25eac51b-b824-4c69-8597-8e06bd9e5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af414bf0-422c-407b-9280-a2acd23d4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(8), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bdde630-4c7e-4853-bfde-8d6f9c1e92c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.496396064758301"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8652a6a-3e13-4e32-9019-59e3b9d836c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ed3668b-ff2f-4432-aea9-4ceef3e1b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e7397-7c6c-4f11-9f6c-b059ca86030a",
   "metadata": {},
   "source": [
    " ### Looping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73d253bb-9f34-4632-a38e-8f1c3427cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  201756\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a83e29a4-40f9-413c-8dd6-1d3d2340fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17fde2f2-61c8-46da-8901-22b4945a4762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 2.2345\n",
      "Step 1: Loss = 2.2344\n",
      "Step 2: Loss = 2.2344\n",
      "Step 3: Loss = 2.2344\n",
      "Step 4: Loss = 2.2344\n",
      "Step 5: Loss = 2.2344\n",
      "Step 6: Loss = 2.2344\n",
      "Step 7: Loss = 2.2344\n",
      "Step 8: Loss = 2.2344\n",
      "Step 9: Loss = 2.2344\n",
      "Step 10: Loss = 2.2344\n",
      "Step 11: Loss = 2.2344\n",
      "Step 12: Loss = 2.2344\n",
      "Step 13: Loss = 2.2344\n",
      "Step 14: Loss = 2.2343\n",
      "Step 15: Loss = 2.2343\n",
      "Step 16: Loss = 2.2343\n",
      "Step 17: Loss = 2.2343\n",
      "Step 18: Loss = 2.2343\n",
      "Step 19: Loss = 2.2343\n",
      "Step 20: Loss = 2.2343\n",
      "Step 21: Loss = 2.2343\n",
      "Step 22: Loss = 2.2343\n",
      "Step 23: Loss = 2.2343\n",
      "Step 24: Loss = 2.2343\n",
      "Step 25: Loss = 2.2343\n",
      "Step 26: Loss = 2.2343\n",
      "Step 27: Loss = 2.2342\n",
      "Step 28: Loss = 2.2342\n",
      "Step 29: Loss = 2.2342\n",
      "Step 30: Loss = 2.2342\n",
      "Step 31: Loss = 2.2342\n",
      "Step 32: Loss = 2.2342\n",
      "Step 33: Loss = 2.2342\n",
      "Step 34: Loss = 2.2342\n",
      "Step 35: Loss = 2.2342\n",
      "Step 36: Loss = 2.2342\n",
      "Step 37: Loss = 2.2342\n",
      "Step 38: Loss = 2.2342\n",
      "Step 39: Loss = 2.2342\n",
      "Step 40: Loss = 2.2342\n",
      "Step 41: Loss = 2.2342\n",
      "Step 42: Loss = 2.2342\n",
      "Step 43: Loss = 2.2341\n",
      "Step 44: Loss = 2.2341\n",
      "Step 45: Loss = 2.2341\n",
      "Step 46: Loss = 2.2341\n",
      "Step 47: Loss = 2.2341\n",
      "Step 48: Loss = 2.2341\n",
      "Step 49: Loss = 2.2341\n",
      "Step 50: Loss = 2.2341\n",
      "Step 51: Loss = 2.2341\n",
      "Step 52: Loss = 2.2341\n",
      "Step 53: Loss = 2.2341\n",
      "Step 54: Loss = 2.2341\n",
      "Step 55: Loss = 2.2341\n",
      "Step 56: Loss = 2.2341\n",
      "Step 57: Loss = 2.2341\n",
      "Step 58: Loss = 2.2341\n",
      "Step 59: Loss = 2.2341\n",
      "Step 60: Loss = 2.2340\n",
      "Step 61: Loss = 2.2340\n",
      "Step 62: Loss = 2.2340\n",
      "Step 63: Loss = 2.2340\n",
      "Step 64: Loss = 2.2340\n",
      "Step 65: Loss = 2.2340\n",
      "Step 66: Loss = 2.2340\n",
      "Step 67: Loss = 2.2340\n",
      "Step 68: Loss = 2.2340\n",
      "Step 69: Loss = 2.2340\n",
      "Step 70: Loss = 2.2340\n",
      "Step 71: Loss = 2.2340\n",
      "Step 72: Loss = 2.2340\n",
      "Step 73: Loss = 2.2340\n",
      "Step 74: Loss = 2.2340\n",
      "Step 75: Loss = 2.2340\n",
      "Step 76: Loss = 2.2340\n",
      "Step 77: Loss = 2.2340\n",
      "Step 78: Loss = 2.2340\n",
      "Step 79: Loss = 2.2340\n",
      "Step 80: Loss = 2.2340\n",
      "Step 81: Loss = 2.2339\n",
      "Step 82: Loss = 2.2339\n",
      "Step 83: Loss = 2.2339\n",
      "Step 84: Loss = 2.2339\n",
      "Step 85: Loss = 2.2339\n",
      "Step 86: Loss = 2.2339\n",
      "Step 87: Loss = 2.2339\n",
      "Step 88: Loss = 2.2339\n",
      "Step 89: Loss = 2.2339\n",
      "Step 90: Loss = 2.2339\n",
      "Step 91: Loss = 2.2339\n",
      "Step 92: Loss = 2.2339\n",
      "Step 93: Loss = 2.2339\n",
      "Step 94: Loss = 2.2339\n",
      "Step 95: Loss = 2.2339\n",
      "Step 96: Loss = 2.2339\n",
      "Step 97: Loss = 2.2339\n",
      "Step 98: Loss = 2.2339\n",
      "Step 99: Loss = 2.2339\n"
     ]
    }
   ],
   "source": [
    "# Number of training samples\n",
    "num = xs.shape[0]\n",
    "\n",
    "# Gradient descent loop\n",
    "for k in range(100):\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()      # One-hot encode inputs\n",
    "    logits = xenc @ W                                 # Compute raw scores (log-counts)\n",
    "    counts = logits.exp()                             # Convert to positive counts\n",
    "    probs = counts / counts.sum(1, keepdim=True)      # Softmax to get probabilities\n",
    "\n",
    "    # Loss: negative log likelihood + L2 regularization\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    print(f\"Step {k}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None                                     # Clear previous gradients\n",
    "    loss.backward()                                   # Compute new gradients\n",
    "\n",
    "    # Update weights (manual SGD step)\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04e471f4-b1af-4297-9b3f-979d2ec1479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deend.\n",
      "poml.\n",
      "urarohakay.\n",
      "n.\n",
      "pojimititinjakavi.\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the trained model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0  # Start with special character '.'\n",
    "\n",
    "    while True:\n",
    "        # One-hot encode the current character index\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "\n",
    "        # Forward pass through the \"network\"\n",
    "        logits = xenc @ W                         # Raw scores\n",
    "        counts = logits.exp()                     # Convert to positive counts\n",
    "        p = counts / counts.sum(1, keepdim=True)  # Softmax to get probabilities\n",
    "\n",
    "        # Sample next character index from the probability distribution\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        # Append the sampled character\n",
    "        out.append(itos[ix])\n",
    "\n",
    "        # Stop if end-of-word token ('.') is generated\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    # Print the generated word\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6dd4d-632d-4f1c-8a7e-59101ff807a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:backtest] *",
   "language": "python",
   "name": "conda-env-backtest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
